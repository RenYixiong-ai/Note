{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划算法\n",
    "这部分算法包含两部分：策略迭代算法和价值迭代算法。\n",
    "\n",
    "## 策略迭代算法\n",
    "### 第一步：状态价值评估\n",
    "从状态价值最优Bellman方程出发：\n",
    "\n",
    "$V^{\\pi}(s)=\\sum_{a \\in A}\\pi(a|s)\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{\\pi}(s')\\right)$\n",
    "\n",
    "可以将策略价值函数改写为一个递归的形式：\n",
    "\n",
    "$V^{k+1}(s)=\\sum_{a \\in A}\\pi(a|s)\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{k}(s')\\right)$\n",
    "\n",
    "其中$k$表示经过$k$次迭代的策略价值函数。\n",
    "\n",
    "### 第二步：策略提升\n",
    "\n",
    "策略提升定理：$Q^{\\pi}(s, \\pi' (s))\\geq V^{\\pi}(s)$，可以说明策略$\\pi'$更优。\n",
    "\n",
    "用贪心的方法确定新的策略：\n",
    "$\\pi' (s) = \\text{argmax}_a Q^{\\pi}(s, a) = \\text{argmax}_a \\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{k}(s)\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 构建悬崖行走的环境\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, nrow=4, ncolumn=12):\n",
    "        # 总共具有的位置为nrow、ncolumn，每个位置存储动作数组，每个动作存储可能的到达的状态，状态包含四个信息（概率、新状态、奖励、done）\n",
    "        self.nrow = nrow        #定义行\n",
    "        self.ncolumn = ncolumn  #定义列\n",
    "        self.P = self.createP()\n",
    "    \n",
    "    def createP(self):\n",
    "        action = [[0, -1], [1, 0], [0, 1], [-1, 0]]\n",
    "        P = np.zeros([self.nrow*self.ncolumn, len(action), 4])      #这样设计不合理，因为状态因该是一个整数\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncolumn):\n",
    "                for l, _ in enumerate(action):\n",
    "                    done = 0\n",
    "                    reward = 0\n",
    "                    new_row = min(self.nrow-1, max(0, i+action[l][0]))\n",
    "                    new_column = min(self.ncolumn-1, max(0, j+action[l][1]))\n",
    "                    #设置终止位置(不包含奖励终止)\n",
    "                    if i == self.nrow-1 and j>0 and j!=self.ncolumn-1:\n",
    "                        done = 1\n",
    "                        reward = 0\n",
    "                        P[i*self.ncolumn + j, l] = [1, int(i*self.ncolumn + j), reward, done]\n",
    "                        continue\n",
    "                    #设置奖励\n",
    "                    if new_row == self.nrow-1 and new_column == self.ncolumn-1:\n",
    "                        reward = 1\n",
    "                        done = 1\n",
    "                    state = new_row*self.ncolumn + new_column\n",
    "                    P[i*self.ncolumn + j, l] = [1, int(state), reward, done]\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#策略价值函数迭代计算\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env, gamma, theta):\n",
    "        self.env = env\n",
    "        self.gamma = gamma      #折扣比例\n",
    "        self.theta = theta      #收敛值\n",
    "        self.V = np.zeros(env.nrow * env.ncolumn)\n",
    "        self.Pi = np.zeros([env.nrow * env.ncolumn, 4]) + 0.25\n",
    "\n",
    "    #策略评估\n",
    "    def Policy_evaluation(self):\n",
    "        while True:\n",
    "            #一次迭代过程\n",
    "            new_V = np.zeros(self.env.nrow * self.env.ncolumn)\n",
    "            for state in range(self.env.nrow * self.env.ncolumn):\n",
    "                action = [[0, -1], [1, 0], [0, 1], [-1, 0]]\n",
    "                for num_act, act in enumerate(action):\n",
    "                    #这部分因该包含一个针对所有可能情况的循环，这里每个动作只有一个确定的结果，因此没有循环\n",
    "                    p, new_state, reward, done = self.env.P[state, num_act]\n",
    "                    new_V[state] += self.Pi[state, num_act] * (reward + self.gamma*p*self.V[int(new_state)])\n",
    "            # 计算两次迭代的收敛差距\n",
    "            if np.max(np.abs(self.V-new_V))<self.theta:\n",
    "                break\n",
    "            #更新状态价值函数\n",
    "            self.V = np.copy(new_V)\n",
    "        return new_V\n",
    "\n",
    "    #策略提升\n",
    "    def Policy_improvement(self):\n",
    "        for state in range(self.env.nrow * self.env.ncolumn):\n",
    "            # 得到每个新状态的状态价值函数\n",
    "            act_state_value = np.zeros(4)\n",
    "            for act in range(4):\n",
    "                _, new_stat, _, _ = self.env.P[state, act]\n",
    "                act_state_value[act] = self.V[int(new_stat)]\n",
    "            # 将动作的状态价值函数归一化，作为新的策略\n",
    "            if np.sum(act_state_value) > 0.01:\n",
    "                act_state_value = act_state_value/np.sum(act_state_value)\n",
    "                self.Pi[state] = act_state_value\n",
    "        return self.Pi\n",
    "\n",
    "    #策略迭代\n",
    "    def Policy_iteration(self):\n",
    "        while True:\n",
    "            self.Policy_evaluation()\n",
    "            old_Pi = np.copy(self.Pi)\n",
    "            self.Policy_improvement()\n",
    "            if np.max(np.abs(old_Pi - self.Pi))==0 : break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05094753 0.06768305 0.09613392 0.1374329  0.19617365 0.27878639\n",
      " 0.39332695 0.54908521 0.75470257 1.0128891  1.30616783 1.56135029\n",
      " 0.05274633 0.07163258 0.10258081 0.14774014 0.2128749  0.30627885\n",
      " 0.43947875 0.62795746 0.89169909 1.25242685 1.71972427 2.22846918\n",
      " 0.05390552 0.07835528 0.11320432 0.16400502 0.23791368 0.34538327\n",
      " 0.5016966  0.7298125  1.06448268 1.5598615  2.30204277 3.43022506\n",
      " 0.04226094 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         4.53290339]\n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "agent = PolicyIteration(env, 0.9, 0.001)\n",
    "agent.Policy_iteration()\n",
    "print(agent.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值迭代算法\n",
    "以贪心的方式，直接对价值函数进行迭代。\n",
    "\n",
    "从状态价值最优Bellman方程出发：\n",
    "\n",
    "$V^{\\pi}(s)=\\text{max}_{a \\in A}\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{\\pi}(s')\\right)$\n",
    "\n",
    "进而改写成递归的形式：\n",
    "\n",
    "$V^{k+1}(s)=\\text{max}_{a \\in A}\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{k}(s')\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 价值迭代函数\n",
    "class ValueIteration:\n",
    "    def __init__(self, env, gamma):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.V = np.zeros(env.nrow * env.ncolumn)\n",
    "        self.Pi = self.Pi = np.zeros([env.nrow * env.ncolumn, 4])\n",
    "\n",
    "    def value_iteration(self):\n",
    "        old_V = np.zeros(env.nrow * env.ncolumn)\n",
    "        while True:\n",
    "            # 一次迭代的过程\n",
    "            for state in range(self.env.nrow * self.env.ncolumn):\n",
    "                action = [[0, -1], [1, 0], [0, 1], [-1, 0]]\n",
    "                new_V = np.zeros(len(action))\n",
    "                for num_act, act in enumerate(action):\n",
    "                    p, new_state, reward, _ = self.env.P[state, num_act]\n",
    "                    new_V[num_act] = reward+self.gamma*p*self.V[int(new_state)]\n",
    "                self.V[state] = np.max(new_V)\n",
    "            # 判断是否收敛\n",
    "            if np.max(self.V - old_V) == 0: break\n",
    "            else: old_V = np.copy(self.V)\n",
    "        return self.V\n",
    "\n",
    "    #利用状态价值函数，产生贪婪策略\n",
    "    def get_policy(self):\n",
    "        self.value_iteration()\n",
    "        for state in range(self.env.nrow * self.env.ncolumn):\n",
    "            state_value = np.zeros(4)\n",
    "            for num_act in range(4):\n",
    "                _, new_state, _, _ = self.env.P[state, num_act]\n",
    "                state_value[num_act] = self.V[int(new_state)]\n",
    "            self.Pi[state, np.argmax(state_value)] = 1  #这里只返回靠前的下标，最终的策略只是最优策略的一种\n",
    "        return self.Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "agent = ValueIteration(env, 0.9)\n",
    "agent.get_policy()\n",
    "\n",
    "print(agent.Pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RL_jidi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56aa5d81d4959884d9009b0c5239d0923ad1f8132d8ef306540209260b17049c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
