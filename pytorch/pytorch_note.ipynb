{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "本文是B站课程[《PyTorch深度学习实践》完结合集](https://www.bilibili.com/video/BV1Y7411d7Ys/?spm_id_from=333.999.0.0&vd_source=f9eb99d14a0acbcfa188c1e70864412e)的笔记，以及后期自己使用学习的补充，其中包括的内容为：\n",
    "\n",
    "* torch 的数组构建，以及反向传播的实现\n",
    "* 标准的全连接网络结构\n",
    "* 数据集Dataset、DataLoader\n",
    "* 卷积神经网络实现\n",
    "* 循环神经网络\n",
    "* 模型保存与读取\n",
    "\n",
    "以下是必要引入的库函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                        \n",
    "import numpy as np      #熟悉的数据处理工具\n",
    "import matplotlib.pyplot as plt     #画图工具\n",
    "from torch.utils.data import Dataset        #读取数据集 \n",
    "from torch.utils.data import DataLoader     #将数据集以mini-batch的方式进行训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch 数组构建\n",
    "torch 构建的数组，分为两个部分，一个是数据的数组，另一部分是保存梯度的数组。默认梯度数组是不构建的，需要手动选择。\n",
    "反向传播的过程中需要定义损失函数，而且在之后需要将梯度清零，否则将会与之前的梯度相加。\n",
    "\n",
    "以下考虑任务：拟合$y = w x$中的$w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 40.0\n",
      "\tgrad: 1.0 2.0 tensor([10.]) 16.0\n",
      "\tgrad: 2.0 4.0 tensor([9.8400]) 78.72000122070312\n",
      "\tgrad: 3.0 6.0 tensor([9.0528]) 205.67041015625\n",
      "\tgrad: 1.0 2.0 tensor([6.9961]) 215.66259765625\n",
      "\tgrad: 2.0 4.0 tensor([4.8395]) 238.37835693359375\n",
      "\tgrad: 3.0 6.0 tensor([2.4557]) 246.58070373535156\n",
      "\tgrad: 1.0 2.0 tensor([-0.0101]) 242.56045532226562\n",
      "\tgrad: 2.0 4.0 tensor([-2.4357]) 207.07464599609375\n",
      "\tgrad: 3.0 6.0 tensor([-4.5065]) 89.95816040039062\n",
      "\tgrad: 1.0 2.0 tensor([-5.4061]) 75.14605712890625\n",
      "\tgrad: 2.0 4.0 tensor([-6.1575]) 9.885948181152344\n",
      "\tgrad: 3.0 6.0 tensor([-6.2564]) -138.728759765625\n",
      "\tgrad: 1.0 2.0 tensor([-4.8691]) -152.46693420410156\n",
      "\tgrad: 2.0 4.0 tensor([-3.3444]) -195.22225952148438\n",
      "\tgrad: 3.0 6.0 tensor([-1.3922]) -256.28173828125\n",
      "\tgrad: 1.0 2.0 tensor([1.1706]) -257.94049072265625\n",
      "\tgrad: 2.0 4.0 tensor([3.7500]) -243.9402618408203\n",
      "\tgrad: 3.0 6.0 tensor([6.1894]) -168.53048706054688\n",
      "\tgrad: 1.0 2.0 tensor([7.8747]) -156.78102111816406\n",
      "\tgrad: 2.0 4.0 tensor([9.4425]) -97.24065399169922\n",
      "\tgrad: 3.0 6.0 tensor([10.4150]) 54.228492736816406\n",
      "\tgrad: 1.0 2.0 tensor([9.8727]) 69.97383117675781\n",
      "\tgrad: 2.0 4.0 tensor([9.1729]) 127.35726165771484\n",
      "\tgrad: 3.0 6.0 tensor([7.8994]) 233.54568481445312\n",
      "\tgrad: 1.0 2.0 tensor([5.5639]) 240.67347717285156\n",
      "\tgrad: 2.0 4.0 tensor([3.1572]) 249.93080139160156\n",
      "\tgrad: 3.0 6.0 tensor([0.6579]) 225.77223205566406\n",
      "\tgrad: 1.0 2.0 tensor([-1.5999]) 218.57249450683594\n",
      "\tgrad: 2.0 4.0 tensor([-3.7856]) 172.28778076171875\n",
      "\tgrad: 3.0 6.0 tensor([-5.5085]) 37.13536071777344\n",
      "\tgrad: 1.0 2.0 tensor([-5.8798]) 21.375717163085938\n",
      "\tgrad: 2.0 4.0 tensor([-6.0936]) -43.37290954589844\n",
      "\tgrad: 3.0 6.0 tensor([-5.6598]) -181.2501983642578\n",
      "\tgrad: 1.0 2.0 tensor([-3.8473]) -192.9449005126953\n",
      "\tgrad: 2.0 4.0 tensor([-1.9179]) -224.2880859375\n",
      "\tgrad: 3.0 6.0 tensor([0.3250]) -254.43841552734375\n",
      "\tgrad: 1.0 2.0 tensor([2.8694]) -252.69967651367188\n",
      "\tgrad: 2.0 4.0 tensor([5.3964]) -225.52877807617188\n",
      "\tgrad: 3.0 6.0 tensor([7.6517]) -123.79906463623047\n",
      "\tgrad: 1.0 2.0 tensor([8.8896]) -110.01978302001953\n",
      "\tgrad: 2.0 4.0 tensor([9.9898]) -46.10107421875\n",
      "\tgrad: 3.0 6.0 tensor([10.4508]) 106.01422119140625\n",
      "\tgrad: 1.0 2.0 tensor([9.3907]) 120.79563903808594\n",
      "\tgrad: 2.0 4.0 tensor([8.1828]) 170.2576446533203\n",
      "\tgrad: 3.0 6.0 tensor([6.4802]) 250.90077209472656\n",
      "\tgrad: 1.0 2.0 tensor([3.9712]) 254.84310913085938\n",
      "\tgrad: 2.0 4.0 tensor([1.4227]) 250.22499084472656\n",
      "\tgrad: 3.0 6.0 tensor([-1.0795]) 194.79373168945312\n",
      "\tgrad: 1.0 2.0 tensor([-3.0275]) 184.73883056640625\n",
      "\tgrad: 2.0 4.0 tensor([-4.8748]) 129.7401123046875\n",
      "\tgrad: 3.0 6.0 tensor([-6.1722]) -17.360214233398438\n",
      "\tgrad: 1.0 2.0 tensor([-5.9986]) -33.35749053955078\n",
      "\tgrad: 2.0 4.0 tensor([-5.6651]) -94.6780014038086\n",
      "\tgrad: 3.0 6.0 tensor([-4.7183]) -215.6071014404297\n",
      "\tgrad: 1.0 2.0 tensor([-2.5622]) -224.7315216064453\n",
      "\tgrad: 2.0 4.0 tensor([-0.3149]) -243.25070190429688\n",
      "\tgrad: 3.0 6.0 tensor([2.1176]) -241.13372802734375\n",
      "\tgrad: 1.0 2.0 tensor([4.5289]) -236.07583618164062\n",
      "\tgrad: 2.0 4.0 tensor([6.8897]) -196.95819091796875\n",
      "\tgrad: 3.0 6.0 tensor([8.8593]) -73.49102020263672\n",
      "\tgrad: 1.0 2.0 tensor([9.5942]) -58.30262756347656\n",
      "\tgrad: 2.0 4.0 tensor([10.1772]) 7.1151580810546875\n",
      "\tgrad: 3.0 6.0 tensor([10.1061]) 153.02444458007812\n",
      "\tgrad: 1.0 2.0 tensor([8.5758]) 166.1761016845703\n",
      "\tgrad: 2.0 4.0 tensor([6.9141]) 205.48863220214844\n",
      "\tgrad: 3.0 6.0 tensor([4.8592]) 256.953857421875\n",
      "\tgrad: 1.0 2.0 tensor([2.2896]) 257.53314208984375\n",
      "\tgrad: 2.0 4.0 tensor([-0.2857]) 239.24761962890625\n",
      "\tgrad: 3.0 6.0 tensor([-2.6782]) 155.04061889648438\n",
      "\tgrad: 1.0 2.0 tensor([-4.2286]) 142.58346557617188\n",
      "\tgrad: 2.0 4.0 tensor([-5.6544]) 81.34820556640625\n",
      "\tgrad: 3.0 6.0 tensor([-6.4679]) -71.07380676269531\n",
      "\tgrad: 1.0 2.0 tensor([-5.7572]) -86.5881118774414\n",
      "\tgrad: 2.0 4.0 tensor([-4.8913]) -141.71827697753906\n",
      "\tgrad: 3.0 6.0 tensor([-3.4741]) -240.25186157226562\n",
      "\tgrad: 1.0 2.0 tensor([-1.0716]) -246.39500427246094\n",
      "\tgrad: 2.0 4.0 tensor([1.3924]) -251.25596618652344\n",
      "\tgrad: 3.0 6.0 tensor([3.9049]) -216.967041015625\n",
      "\tgrad: 1.0 2.0 tensor([6.0746]) -208.8178253173828\n",
      "\tgrad: 2.0 4.0 tensor([8.1628]) -159.51551818847656\n",
      "\tgrad: 3.0 6.0 tensor([9.7579]) -19.872543334960938\n",
      "\tgrad: 1.0 2.0 tensor([9.9567]) -3.9592056274414062\n",
      "\tgrad: 2.0 4.0 tensor([9.9963]) 60.01087951660156\n",
      "\tgrad: 3.0 6.0 tensor([9.3962]) 193.1416015625\n",
      "\tgrad: 1.0 2.0 tensor([7.4647]) 204.07107543945312\n",
      "\tgrad: 2.0 4.0 tensor([5.4240]) 231.46327209472656\n",
      "\tgrad: 3.0 6.0 tensor([3.1094]) 251.4323272705078\n",
      "\tgrad: 1.0 2.0 tensor([0.5951]) 248.62246704101562\n",
      "\tgrad: 2.0 4.0 tensor([-1.8912]) 217.49322509765625\n",
      "\tgrad: 3.0 6.0 tensor([-4.0661]) 108.30363464355469\n",
      "\tgrad: 1.0 2.0 tensor([-5.1491]) 94.00538635253906\n",
      "\tgrad: 2.0 4.0 tensor([-6.0892]) 29.291961669921875\n",
      "\tgrad: 3.0 6.0 tensor([-6.3821]) -121.58580017089844\n",
      "\tgrad: 1.0 2.0 tensor([-5.1662]) -135.91827392578125\n",
      "\tgrad: 2.0 4.0 tensor([-3.8071]) -182.37472534179688\n",
      "\tgrad: 3.0 6.0 tensor([-1.9833]) -254.07431030273438\n",
      "\tgrad: 1.0 2.0 tensor([0.5574]) -256.9594421386719\n",
      "\tgrad: 2.0 4.0 tensor([3.1270]) -247.94322204589844\n",
      "\tgrad: 3.0 6.0 tensor([5.6065]) -183.02694702148438\n",
      "\tgrad: 1.0 2.0 tensor([7.4367]) -172.1534881591797\n",
      "\tgrad: 2.0 4.0 tensor([9.1583]) -114.88737487792969\n",
      "\tgrad: 3.0 6.0 tensor([10.3071]) 34.64109802246094\n",
      "\tgrad: 1.0 2.0 tensor([9.9607]) 50.56255340576172\n",
      "\tgrad: 2.0 4.0 tensor([9.4551]) 110.20336151123047\n",
      "\tgrad: 3.0 6.0 tensor([8.3531]) 224.55856323242188\n",
      "\tgrad: 1.0 2.0 tensor([6.1075]) 232.77352905273438\n",
      "\tgrad: 2.0 4.0 tensor([3.7797]) 247.01150512695312\n",
      "\tgrad: 3.0 6.0 tensor([1.3096]) 234.5848846435547\n",
      "\tgrad: 1.0 2.0 tensor([-1.0362]) 228.512451171875\n",
      "\tgrad: 2.0 4.0 tensor([-3.3213]) 185.94171142578125\n",
      "\tgrad: 3.0 6.0 tensor([-5.1808]) 56.68806457519531\n",
      "\tgrad: 1.0 2.0 tensor([-5.7476]) 41.192787170410156\n",
      "\tgrad: 2.0 4.0 tensor([-6.1596]) -24.08374786376953\n",
      "\tgrad: 3.0 6.0 tensor([-5.9187]) -166.62088012695312\n",
      "\tgrad: 1.0 2.0 tensor([-4.2525]) -179.12591552734375\n",
      "\tgrad: 2.0 4.0 tensor([-2.4613]) -214.81600952148438\n",
      "\tgrad: 3.0 6.0 tensor([-0.3131]) -256.45184326171875\n",
      "\tgrad: 1.0 2.0 tensor([2.2514]) -255.94900512695312\n",
      "\tgrad: 2.0 4.0 tensor([4.8109]) -233.4617462158203\n",
      "\tgrad: 3.0 6.0 tensor([7.1455]) -140.84230041503906\n",
      "\tgrad: 1.0 2.0 tensor([8.5539]) -127.73440551757812\n",
      "\tgrad: 2.0 4.0 tensor([9.8313]) -65.08407592773438\n",
      "\tgrad: 3.0 6.0 tensor([10.4821]) 87.59429931640625\n",
      "\tgrad: 1.0 2.0 tensor([9.6062]) 102.80667877197266\n",
      "\tgrad: 2.0 4.0 tensor([8.5781]) 155.43165588378906\n",
      "\tgrad: 3.0 6.0 tensor([7.0238]) 245.8601531982422\n",
      "\tgrad: 1.0 2.0 tensor([4.5652]) 250.9905548095703\n",
      "\tgrad: 2.0 4.0 tensor([2.0553]) 251.4329376220703\n",
      "\tgrad: 3.0 6.0 tensor([-0.4590]) 207.17037963867188\n",
      "\tgrad: 1.0 2.0 tensor([-2.5307]) 198.10891723632812\n",
      "\tgrad: 2.0 4.0 tensor([-4.5118]) 146.0143280029297\n",
      "\tgrad: 3.0 6.0 tensor([-5.9720]) 2.5189208984375\n",
      "\tgrad: 1.0 2.0 tensor([-5.9972]) -13.475391387939453\n",
      "\tgrad: 2.0 4.0 tensor([-5.8624]) -76.3746109008789\n",
      "\tgrad: 3.0 6.0 tensor([-5.0987]) -204.15042114257812\n",
      "\tgrad: 1.0 2.0 tensor([-3.0572]) -214.2647247314453\n",
      "\tgrad: 2.0 4.0 tensor([-0.9145]) -237.5807647705078\n",
      "\tgrad: 3.0 6.0 tensor([1.4613]) -247.27731323242188\n",
      "\tgrad: 1.0 2.0 tensor([3.9341]) -243.40916442871094\n",
      "\tgrad: 2.0 4.0 tensor([6.3682]) -208.4638214111328\n",
      "\tgrad: 3.0 6.0 tensor([8.4528]) -92.31331634521484\n",
      "\tgrad: 1.0 2.0 tensor([9.3759]) -77.56143951416016\n",
      "\tgrad: 2.0 4.0 tensor([10.1516]) -12.349014282226562\n",
      "\tgrad: 3.0 6.0 tensor([10.2750]) 136.6017608642578\n",
      "\tgrad: 1.0 2.0 tensor([8.9090]) 150.41981506347656\n",
      "\tgrad: 2.0 4.0 tensor([7.4048]) 193.658447265625\n",
      "\tgrad: 3.0 6.0 tensor([5.4682]) 256.08685302734375\n",
      "\tgrad: 1.0 2.0 tensor([2.9074]) 257.901611328125\n",
      "\tgrad: 2.0 4.0 tensor([0.3284]) 244.5284881591797\n",
      "\tgrad: 3.0 6.0 tensor([-2.1169]) 170.423828125\n",
      "\tgrad: 1.0 2.0 tensor([-3.8212]) 158.781494140625\n",
      "\tgrad: 2.0 4.0 tensor([-5.4090]) 99.50965881347656\n",
      "\tgrad: 3.0 6.0 tensor([-6.4041]) -51.763702392578125\n",
      "\tgrad: 1.0 2.0 tensor([-5.8864]) -67.53657531738281\n",
      "\tgrad: 2.0 4.0 tensor([-5.2111]) -125.22515869140625\n",
      "\tgrad: 3.0 6.0 tensor([-3.9588]) -232.4839324951172\n",
      "\tgrad: 1.0 2.0 tensor([-1.6340]) -239.75189208984375\n",
      "\tgrad: 2.0 4.0 tensor([0.7635]) -249.64358520507812\n",
      "\tgrad: 3.0 6.0 tensor([3.2600]) -226.9640655517578\n",
      "\tgrad: 1.0 2.0 tensor([5.5296]) -219.9048309326172\n",
      "\tgrad: 2.0 4.0 tensor([7.7287]) -174.07553100585938\n",
      "\tgrad: 3.0 6.0 tensor([9.4694]) -39.626007080078125\n",
      "\tgrad: 1.0 2.0 tensor([9.8657]) -23.894651412963867\n",
      "\tgrad: 2.0 4.0 tensor([10.1046]) 40.942344665527344\n",
      "\tgrad: 3.0 6.0 tensor([9.6952]) 179.45596313476562\n",
      "\tgrad: 1.0 2.0 tensor([7.9006]) 191.2572479248047\n",
      "\tgrad: 2.0 4.0 tensor([5.9881]) 223.16180419921875\n",
      "\tgrad: 3.0 6.0 tensor([3.7565]) 254.77792358398438\n",
      "\tgrad: 1.0 2.0 tensor([1.2087]) 253.1952667236328\n",
      "\tgrad: 2.0 4.0 tensor([-1.3233]) 226.60902404785156\n",
      "\tgrad: 3.0 6.0 tensor([-3.5894]) 126.00035095214844\n",
      "\tgrad: 1.0 2.0 tensor([-4.8494]) 112.30160522460938\n",
      "\tgrad: 2.0 4.0 tensor([-5.9724]) 48.522483825683594\n",
      "\tgrad: 3.0 6.0 tensor([-6.4576]) -103.7145767211914\n",
      "\tgrad: 1.0 2.0 tensor([-5.4205]) -118.55551147460938\n",
      "\tgrad: 2.0 4.0 tensor([-4.2349]) -168.43482971191406\n",
      "\tgrad: 3.0 6.0 tensor([-2.5506]) -250.34503173828125\n",
      "\tgrad: 1.0 2.0 tensor([-0.0471]) -254.43927001953125\n",
      "\tgrad: 2.0 4.0 tensor([2.4973]) -250.4610595703125\n",
      "\tgrad: 3.0 6.0 tensor([5.0019]) -196.42709350585938\n",
      "\tgrad: 1.0 2.0 tensor([6.9662]) -186.49478149414062\n",
      "\tgrad: 2.0 4.0 tensor([8.8311]) -131.84593200683594\n",
      "\tgrad: 3.0 6.0 tensor([10.1496]) 14.846237182617188\n",
      "\tgrad: 1.0 2.0 tensor([10.0011]) 30.84844398498535\n",
      "\tgrad: 2.0 4.0 tensor([9.6926]) 92.38939666748047\n",
      "\tgrad: 3.0 6.0 tensor([8.7687]) 214.2264404296875\n",
      "\tgrad: 1.0 2.0 tensor([6.6265]) 223.4793701171875\n",
      "\tgrad: 2.0 4.0 tensor([4.3917]) 242.61270141601562\n",
      "\tgrad: 3.0 6.0 tensor([1.9655]) 241.992431640625\n",
      "\tgrad: 1.0 2.0 tensor([-0.4544]) 237.0836639404297\n",
      "\tgrad: 2.0 4.0 tensor([-2.8252]) 198.48190307617188\n",
      "\tgrad: 3.0 6.0 tensor([-4.8100]) 75.90119171142578\n",
      "\tgrad: 1.0 2.0 tensor([-5.5691]) 60.76308822631836\n",
      "\tgrad: 2.0 4.0 tensor([-6.1767]) -4.650363922119141\n",
      "\tgrad: 3.0 6.0 tensor([-6.1302]) -150.9935760498047\n",
      "\tgrad: 1.0 2.0 tensor([-4.6202]) -164.2340545654297\n",
      "\tgrad: 2.0 4.0 tensor([-2.9779]) -204.05726623535156\n",
      "\tgrad: 3.0 6.0 tensor([-0.9373]) -256.92919921875\n",
      "\tgrad: 1.0 2.0 tensor([1.6320]) -257.665283203125\n",
      "\tgrad: 2.0 4.0 tensor([4.2086]) -239.99636840820312\n",
      "\tgrad: 3.0 6.0 tensor([6.6086]) -157.0419464111328\n",
      "\tgrad: 1.0 2.0 tensor([8.1790]) -144.68394470214844\n",
      "\tgrad: 2.0 4.0 tensor([9.6258]) -83.67724609375\n",
      "\tgrad: 3.0 6.0 tensor([10.4626]) 68.64973449707031\n",
      "\tgrad: 1.0 2.0 tensor([9.7761]) 84.20195770263672\n",
      "\tgrad: 2.0 4.0 tensor([8.9341]) 139.67471313476562\n",
      "\tgrad: 3.0 6.0 tensor([7.5373]) 239.34695434570312\n",
      "\tgrad: 1.0 2.0 tensor([5.1439]) 245.63470458984375\n",
      "\tgrad: 2.0 4.0 tensor([2.6875]) 251.13494873046875\n",
      "\tgrad: 3.0 6.0 tensor([0.1762]) 218.3061981201172\n",
      "\tgrad: 1.0 2.0 tensor([-2.0069]) 210.2924346923828\n",
      "\tgrad: 2.0 4.0 tensor([-4.1098]) 161.4139862060547\n",
      "\tgrad: 3.0 6.0 tensor([-5.7239]) 22.382965087890625\n",
      "\tgrad: 1.0 2.0 tensor([-5.9478]) 6.487414360046387\n",
      "\tgrad: 2.0 4.0 tensor([-6.0126]) -57.6137809753418\n",
      "\tgrad: 3.0 6.0 tensor([-5.4365]) -191.4709930419922\n",
      "\tgrad: 1.0 2.0 tensor([-3.5218]) -202.5146026611328\n",
      "\tgrad: 2.0 4.0 tensor([-1.4967]) -230.48785400390625\n",
      "\tgrad: 3.0 6.0 tensor([0.8082]) -251.93984985351562\n",
      "\tgrad: 1.0 2.0 tensor([3.3276]) -249.28460693359375\n",
      "\tgrad: 2.0 4.0 tensor([5.8205]) -218.7208709716797\n",
      "\tgrad: 3.0 6.0 tensor([8.0077]) -110.58271789550781\n",
      "\tgrad: 1.0 2.0 tensor([9.1135]) -96.355712890625\n",
      "\tgrad: 2.0 4.0 tensor([10.0771]) -31.739234924316406\n",
      "\tgrad: 3.0 6.0 tensor([10.3945]) 119.36090850830078\n",
      "\tgrad: 1.0 2.0 tensor([9.2008]) 133.76258850097656\n",
      "\tgrad: 2.0 4.0 tensor([7.8632]) 180.66831970214844\n",
      "\tgrad: 3.0 6.0 tensor([6.0565]) 253.68592834472656\n",
      "\tgrad: 1.0 2.0 tensor([3.5197]) 256.72528076171875\n",
      "\tgrad: 2.0 4.0 tensor([0.9524]) 248.3446502685547\n",
      "\tgrad: 3.0 6.0 tensor([-1.5310]) 184.7862091064453\n",
      "\tgrad: 1.0 2.0 tensor([-3.3789]) 174.0284423828125\n",
      "\tgrad: 2.0 4.0 tensor([-5.1192]) 117.0750732421875\n",
      "\tgrad: 3.0 6.0 tensor([-6.2899]) -32.143524169921875\n",
      "\tgrad: 1.0 2.0 tensor([-5.9685]) -48.08049774169922\n",
      "\tgrad: 2.0 4.0 tensor([-5.4877]) -107.98194885253906\n",
      "\tgrad: 3.0 6.0 tensor([-4.4079]) -223.32345581054688\n",
      "\tgrad: 1.0 2.0 tensor([-2.1746]) -231.67271423339844\n",
      "\tgrad: 2.0 4.0 tensor([0.1421]) -246.53591918945312\n",
      "\tgrad: 3.0 6.0 tensor([2.6075]) -235.60165405273438\n",
      "\tgrad: 1.0 2.0 tensor([4.9635]) -229.67469787597656\n",
      "\tgrad: 2.0 4.0 tensor([7.2602]) -187.59292602539062\n",
      "\tgrad: 3.0 6.0 tensor([9.1362]) -59.14219665527344\n",
      "\tgrad: 1.0 2.0 tensor([9.7276]) -43.687049865722656\n",
      "\tgrad: 2.0 4.0 tensor([10.1644]) 21.628501892089844\n",
      "\tgrad: 3.0 6.0 tensor([9.9482]) 164.69537353515625\n",
      "\tgrad: 1.0 2.0 tensor([8.3012]) 177.29779052734375\n",
      "\tgrad: 2.0 4.0 tensor([6.5282]) 213.52362060546875\n",
      "\tgrad: 3.0 6.0 tensor([4.3930]) 256.59747314453125\n",
      "\tgrad: 1.0 2.0 tensor([1.8270]) 256.2514953613281\n",
      "\tgrad: 2.0 4.0 tensor([-0.7355]) 234.3675079345703\n",
      "\tgrad: 3.0 6.0 tensor([-3.0792]) 142.9423828125\n",
      "\tgrad: 1.0 2.0 tensor([-4.5086]) 129.92518615722656\n",
      "\tgrad: 2.0 4.0 tensor([-5.8078]) 67.46239471435547\n",
      "\tgrad: 3.0 6.0 tensor([-6.4825]) -85.22211456298828\n",
      "\tgrad: 1.0 2.0 tensor([-5.6303]) -100.48262023925781\n",
      "\tgrad: 2.0 4.0 tensor([-4.6254]) -153.48602294921875\n",
      "\tgrad: 3.0 6.0 tensor([-3.0906]) -245.1162109375\n",
      "\tgrad: 1.0 2.0 tensor([-0.6394]) -250.39501953125\n",
      "\tgrad: 2.0 4.0 tensor([1.8645]) -251.47865295410156\n",
      "\tgrad: 3.0 6.0 tensor([4.3793]) -208.65066528320312\n",
      "\tgrad: 1.0 2.0 tensor([6.4658]) -199.718994140625\n",
      "\tgrad: 2.0 4.0 tensor([8.4630]) -148.0147705078125\n",
      "\tgrad: 3.0 6.0 tensor([9.9432]) -5.03759765625\n",
      "\tgrad: 1.0 2.0 tensor([9.9936]) 10.949506759643555\n",
      "\tgrad: 2.0 4.0 tensor([9.8841]) 74.02196502685547\n",
      "\tgrad: 3.0 6.0 tensor([9.1438]) 202.61102294921875\n",
      "\tgrad: 1.0 2.0 tensor([7.1177]) 212.8464813232422\n",
      "\tgrad: 2.0 4.0 tensor([4.9893]) 236.7605743408203\n",
      "\tgrad: 3.0 6.0 tensor([2.6217]) 247.9503936767578\n",
      "\tgrad: 1.0 2.0 tensor([0.1422]) 244.2346954345703\n",
      "\tgrad: 2.0 4.0 tensor([-2.3002]) 209.83314514160156\n",
      "\tgrad: 3.0 6.0 tensor([-4.3985]) 94.65968322753906\n",
      "\tgrad: 1.0 2.0 tensor([-5.3451]) 79.96943664550781\n",
      "\tgrad: 2.0 4.0 tensor([-6.1448]) 14.810897827148438\n",
      "\tgrad: 3.0 6.0 tensor([-6.2929]) -134.46176147460938\n",
      "\tgrad: 1.0 2.0 tensor([-4.9483]) -148.35838317871094\n",
      "\tgrad: 2.0 4.0 tensor([-3.4647]) -192.076171875\n",
      "\tgrad: 3.0 6.0 tensor([-1.5440]) -255.86749267578125\n",
      "\tgrad: 1.0 2.0 tensor([1.0147]) -257.83807373046875\n",
      "\tgrad: 2.0 4.0 tensor([3.5931]) -245.09332275390625\n",
      "\tgrad: 3.0 6.0 tensor([6.0440]) -172.30084228515625\n",
      "\tgrad: 1.0 2.0 tensor([7.7670]) -160.7667694091797\n",
      "\tgrad: 2.0 4.0 tensor([9.3747]) -101.76914978027344\n",
      "\tgrad: 3.0 6.0 tensor([10.3924]) 49.29393005371094\n",
      "\tgrad: 1.0 2.0 tensor([9.8995]) 65.09284210205078\n",
      "\tgrad: 2.0 4.0 tensor([9.2485]) 123.0810546875\n",
      "\tgrad: 3.0 6.0 tensor([8.0177]) 231.39996337890625\n",
      "\tgrad: 1.0 2.0 tensor([5.7037]) 238.80740356445312\n",
      "\tgrad: 2.0 4.0 tensor([3.3156]) 249.33255004882812\n",
      "\tgrad: 3.0 6.0 tensor([0.8223]) 228.13426208496094\n",
      "\tgrad: 1.0 2.0 tensor([-1.4590]) 221.21621704101562\n",
      "\tgrad: 2.0 4.0 tensor([-3.6712]) 175.84671020507812\n",
      "\tgrad: 3.0 6.0 tensor([-5.4297]) 42.11293029785156\n",
      "\tgrad: 1.0 2.0 tensor([-5.8508]) 26.41136360168457\n",
      "\tgrad: 2.0 4.0 tensor([-6.1149]) -38.5078125\n",
      "\tgrad: 3.0 6.0 tensor([-5.7298]) -177.64456176757812\n",
      "\tgrad: 1.0 2.0 tensor([-3.9534]) -189.55130004882812\n",
      "\tgrad: 2.0 4.0 tensor([-2.0579]) -222.01419067382812\n",
      "\tgrad: 3.0 6.0 tensor([0.1623]) -255.09312438964844\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([10.0])            # w为需要优化的参数，首先赋予初值\n",
    "w.requires_grad = True              # 打开梯度开关\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# 前向传播过程\n",
    "def forward(x):                     \n",
    "    return x * w\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "# tensor的数组，如果只想输出标量值，要用item()，否则将会携带梯度。长期以往，内存占用过高。\n",
    "print(\"predict (before training)\", 4, forward(4).item()) \n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        print('\\tgrad:', x, y, w.data, w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.data            # 利用梯度数据更新\n",
    "        w.grad.data.zero_()             # 将梯度置零，否则将会与之前的梯度叠加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：从其他地方读入的数组需要转化为 float32 的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标准的全连接网络结构\n",
    "\n",
    "应该包含三个部分：\n",
    "* 网络的构建\n",
    "* 损失函数\n",
    "* 优化器（反向传播的过程）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络构建\n",
    "\n",
    "网络结构应该继承自**torch.nn.Module**，其中必须包含两部分：初始函数 **\\_\\_init\\_\\_** 、前向传播 **forward** 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "model = LinearModel()               # 实例化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数、优化器\n",
    "\n",
    "这部分考虑自己的任务是什么（回归、分类），标签的特征是什么，从而选择对应的损失函数与优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下考虑任务：拟合$y = w x$中的$w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0]])\n",
    "y_data = torch.tensor([[3.0], [6.0]])\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(50):\n",
    "    y_pred = model(x_data)              # 得到预测值\n",
    "    loss = criterion(y_pred, y_data)    # 计算损失函数\n",
    "\n",
    "    optimizer.zero_grad()               # 在反向传播前将网络中参数的梯度置0\n",
    "    loss.backward()                     # 损失进行反向传播\n",
    "    optimizer.step()                    # 优化器对每一个参数进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "Dataset是抽象类用于读取全部的数据集，划分输入数据与目标，无法实例化；DataLoader用以设置mini-batch生成的重要参数，可以实例化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)  # 从外部读取\n",
    "        self.len = xy.shape[0]                          #确定数据的个数\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1])      #设置其中一组数据\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]   # 返回的数据\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len     # 数据总长度\n",
    "\n",
    "dataset = DiabetesDataset(PATH)\n",
    "\n",
    "#实例化，并且设置mini-batch的重要参数\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=4) \n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader):     #读取数据\n",
    "        inputs, labels = data\n",
    "        y_pred = model(inputs)\n",
    "        print(y_pred)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        #print(epoch, i, loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "标准化的过程包含五个部分：CBAPD，分别是:Conv, BatchNormal, Activate, Pool, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=2)\n",
    "        self.conv3 = torch.nn.Conv2d(20, 30, kernel_size=3)\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        self.l1 = torch.nn.Linear(120, 80)\n",
    "        self.l2 = torch.nn.Linear(80, 40)\n",
    "        self.l3 = torch.nn.Linear(40, 10)\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        x = F.relu(self.pooling(self.conv1(input)))\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        x = F.relu(self.pooling(self.conv3(x)))\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "\n",
    "这部分暂时不熟悉，暂且仅仅留出代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用RNNCell单元构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用RNN单元构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存与读取\n",
    "\n",
    "参考官网的[教程](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用GPU计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先确认是否有可用的GPU\n",
    "然后选择一个设备，在训练的时候，数据集与模型必须在同一个设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('learntorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22386426b1390bacfc24976a17d143ccc2537dcadf415e43ca1a4443b2ef9cc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
