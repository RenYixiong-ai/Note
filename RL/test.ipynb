{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划算法\n",
    "这部分算法包含两部分：策略迭代算法和价值迭代算法。\n",
    "\n",
    "## 策略迭代算法\n",
    "### 第一步：状态价值评估\n",
    "从状态价值最优Bellman方程出发：\n",
    "\n",
    "$V^{\\pi}(s)=\\sum_{a \\in A}\\pi(a|s)\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{\\pi}(s)\\right)$\n",
    "\n",
    "可以将策略价值函数改写为一个递归的形式：\n",
    "\n",
    "$V^{k+1}(s)=\\sum_{a \\in A}\\pi(a|s)\\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{k}(s)\\right)$\n",
    "\n",
    "其中$k$表示经过$k$次迭代的策略价值函数。\n",
    "\n",
    "### 第二步：策略提升\n",
    "\n",
    "策略提升定理：$Q^{\\pi}(s, \\pi' (s))\\geq V^{\\pi}(s)$，可以说明策略$\\pi'$更优。\n",
    "\n",
    "用贪心的方法确定新的策略：\n",
    "$\\pi' (s) = \\text{argmax}_a Q^{\\pi}(s, a) = \\text{argmax}_a \\left(r(s, a)+\\gamma \\sum_{s' \\in S}P(s'|s, a)V^{k}(s)\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 构建悬崖行走的环境\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, nrow=4, ncolumn=12):\n",
    "        # 总共具有的位置为nrow、ncolumn，每个位置存储动作数组，每个动作存储可能的到达的状态，状态包含四个信息（概率、新状态、奖励、done）\n",
    "        self.nrow = nrow        #定义行\n",
    "        self.ncolumn = ncolumn  #定义列\n",
    "        self.P = self.createP()\n",
    "    \n",
    "    def createP(self):\n",
    "        action = [[0, -1], [1, 0], [0, 1], [-1, 0]]\n",
    "        P = np.zeros([self.nrow*self.ncolumn, len(action), 4])\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncolumn):\n",
    "                for l, _ in enumerate(action):\n",
    "                    done = 0\n",
    "                    reward = 0\n",
    "                    new_row = min(self.nrow-1, max(0, i+action[l][0]))\n",
    "                    new_column = min(self.ncolumn-1, max(0, j+action[l][1]))\n",
    "                    #设置终止位置\n",
    "                    if i == self.nrow-1 and j>0:\n",
    "                        done = 1\n",
    "                        reward = 0\n",
    "                        P[i*self.ncolumn + j, l] = [1, i*self.ncolumn + j, reward, done]\n",
    "                        continue\n",
    "                    #设置奖励\n",
    "                    if new_row == self.nrow-1 and new_column == self.ncolumn-1:\n",
    "                        reward = 1\n",
    "                    state = new_row*self.ncolumn + new_column\n",
    "                    P[i*self.ncolumn + j, l] = [1, state, reward, done]\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#策略价值函数迭代计算\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RL_jidi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56aa5d81d4959884d9009b0c5239d0923ad1f8132d8ef306540209260b17049c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
